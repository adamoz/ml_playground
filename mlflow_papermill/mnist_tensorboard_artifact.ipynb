{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "cuda = torch.cuda.is_available()\n",
    "seed = 1\n",
    "log_interval = 100\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bbdb42f07541df9abc3852b8aafbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c385f00ea54c018765be7e65672814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd31420cf5f470eb6c77f85b16395b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99225e2c93a465cb7502c85bbcd3581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True, \n",
    "        transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, \n",
    "        transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "        batch_size=test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_loader).next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "\n",
    "    def log_weights(self, step):\n",
    "        writer.add_histogram('weights/conv1/weight', model.conv1.weight.data, step)\n",
    "        writer.add_histogram('weights/conv1/bias', model.conv1.bias.data, step)\n",
    "        writer.add_histogram('weights/conv2/weight', model.conv2.weight.data, step)\n",
    "        writer.add_histogram('weights/conv2/bias', model.conv2.bias.data, step)\n",
    "        writer.add_histogram('weights/fc1/weight', model.fc1.weight.data, step)\n",
    "        writer.add_histogram('weights/fc1/bias', model.fc1.bias.data, step)\n",
    "        writer.add_histogram('weights/fc2/weight', model.fc2.weight.data, step)\n",
    "        writer.add_histogram('weights/fc2/bias', model.fc2.bias.data, step)\n",
    "\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = None # Will be used to write TensorBoard events\n",
    "\n",
    "def log_scalar(name, value, step):\n",
    "    \"\"\"Log a scalar value to both MLflow and TensorBoard\"\"\"\n",
    "    writer.add_scalar(name, value, step)\n",
    "    mlflow.log_metric(name, value)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "            step = epoch * len(train_loader) + batch_idx\n",
    "            log_scalar('train_loss', loss.data.item(), step)\n",
    "            model.log_weights(step)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    log_scalar('test_loss', test_loss, step)\n",
    "    log_scalar('test_accuracy', test_accuracy, step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TensorBoard events locally to /tmp/tmput70y1ap\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.130347\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.052099\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.141832\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.097194\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.082489\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.147342\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.022296\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.089414\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.057757\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.072311\n",
      "\n",
      "Test set: Average loss: 4.6732, Accuracy: 9846/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.270308\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.094780\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.027853\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.235094\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.121951\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.002873\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.102097\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.212121\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.234770\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.016217\n",
      "\n",
      "Test set: Average loss: 4.6690, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.217502\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.024349\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.067131\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.071700\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.211421\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.294040\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.094259\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.049298\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.111594\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.015656\n",
      "\n",
      "Test set: Average loss: 4.6703, Accuracy: 9857/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.086182\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.093111\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.164576\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.018149\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.390823\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.193533\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.048365\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.090869\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.086533\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.068130\n",
      "\n",
      "Test set: Average loss: 4.6691, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.161402\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.047593\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.169385\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.147149\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.040977\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.153772\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.013783\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.039786\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.081065\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.053602\n",
      "\n",
      "Test set: Average loss: 4.6643, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.066844\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.085599\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.072352\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.207437\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.961704\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.014040\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.001208\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.125434\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.130806\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.028589\n",
      "\n",
      "Test set: Average loss: 4.6640, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.091208\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.043658\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.004363\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.068023\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.096987\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.129921\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.062774\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.094208\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.211068\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.106286\n",
      "\n",
      "Test set: Average loss: 4.6647, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.311413\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.096384\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.067867\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.190357\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.086999\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.177041\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.250891\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.032062\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.014541\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.162238\n",
      "\n",
      "Test set: Average loss: 4.6630, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.155308\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.072762\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.019547\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.102007\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.102548\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.277988\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.120639\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.125633\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.138942\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.054153\n",
      "\n",
      "Test set: Average loss: 4.6610, Accuracy: 9869/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.185801\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.227698\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.019442\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.938833\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.124554\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.049927\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.051757\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.070391\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.003581\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.104724\n",
      "\n",
      "Test set: Average loss: 4.6629, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Uploading TensorBoard events as a run artifact...\n",
      "\n",
      "Launch TensorBoard with:\n",
      "\n",
      "tensorboard --logdir=file:///home/adam/Code/ml_playground/mlflow_papermill/mlruns/0/fb7af90503fa496f954d497a24cb7f17/artifacts/events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/mlflow-aca447097abf603ea9b9a7196081ebc9950a2bd8/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'batch_size': batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'cuda': cuda,\n",
    "    'seed': seed,\n",
    "    'log_interval': log_interval}\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log our parameters into mlflow\n",
    "    for key, value in args.items():\n",
    "        mlflow.log_param(key, value)\n",
    "\n",
    "    # Create a SummaryWriter to write TensorBoard events locally\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    writer = SummaryWriter(output_dir)\n",
    "    print(\"Writing TensorBoard events locally to %s\\n\" % output_dir)\n",
    "\n",
    "    # Perform the training\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "    # Upload the TensorBoard event logs as a run artifact\n",
    "    print(\"Uploading TensorBoard events as a run artifact...\")\n",
    "    mlflow.log_artifacts(output_dir, artifact_path=\"events\")\n",
    "    print(\"\\nLaunch TensorBoard with:\\n\\ntensorboard --logdir=%s\" %\n",
    "        os.path.join(mlflow.get_artifact_uri(), \"events\"))\n",
    "\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    torch.save(model, os.path.join(output_dir, 'model.pth'))\n",
    "    mlflow.log_artifacts(output_dir, artifact_path=\"models\")\n",
    "    mlflow.log_artifacts('./notebooks', artifact_path=\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
