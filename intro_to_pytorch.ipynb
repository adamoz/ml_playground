{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0919e+10,  4.5804e-41, -4.1639e-34,  3.0701e-41],\n",
       "        [-4.0676e-34,  3.0701e-41, -4.1162e-34,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.4013e-45,  0.0000e+00,  3.3771e-43,  0.0000e+00],\n",
       "        [-4.1168e-34,  3.0701e-41, -4.1172e-34,  3.0701e-41]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0852, 0.3397, 0.1428, 0.8152],\n",
       "        [0.4049, 0.6252, 0.8189, 0.8183],\n",
       "        [0.0893, 0.2852, 0.0370, 0.4465],\n",
       "        [0.2315, 0.8034, 0.2854, 0.8236],\n",
       "        [0.8572, 0.0638, 0.7866, 0.3811]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1635, 0.8862, 0.3673, 0.2578],\n",
       "        [0.4794, 0.2664, 0.6679, 0.9998],\n",
       "        [0.5770, 0.4683, 0.5257, 0.0643],\n",
       "        [0.5916, 0.3146, 0.1784, 0.0361],\n",
       "        [0.3267, 0.2900, 0.1372, 0.4784]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1635279655456543,\n",
       "  0.8862454891204834,\n",
       "  0.36734676361083984,\n",
       "  0.2578318119049072]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16352797,  0.88624549,  0.36734676,  0.25783181],\n",
       "       [ 0.47940516,  0.2664358 ,  0.6679309 ,  0.99979913],\n",
       "       [ 0.57697415,  0.46834332,  0.52573955,  0.06427157],\n",
       "       [ 0.59158486,  0.31462371,  0.17836815,  0.03613251],\n",
       "       [ 0.32673222,  0.28996468,  0.13719589,  0.47843015]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "x = x.to(device)\n",
    "xx = x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grad setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3, 1], requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3.], grad_fn=<AddBackward>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.9221,  9.5707]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y + torch.rand(1, 2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad_(True)\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7464, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient is cumulative\n",
    "x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join existing gradinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50., 30.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3, 1], requires_grad=True, dtype=torch.float)\n",
    "y = x + 2\n",
    "z = y * y + torch.rand(1, 2)\n",
    "out = z.mean()\n",
    "\n",
    "\n",
    "ready_gradient = torch.tensor(10, dtype=torch.float)\n",
    "out.backward(ready_gradient)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    #optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('input_linear.weight', Parameter containing:\n",
       "  tensor([[ 0.0061,  0.0238, -0.0026,  ...,  0.0098, -0.0177,  0.0157],\n",
       "          [-0.0202, -0.0154, -0.0319,  ...,  0.0318,  0.0074,  0.0169],\n",
       "          [-0.0454,  0.0056,  0.0340,  ..., -0.0133,  0.0121, -0.0135],\n",
       "          ...,\n",
       "          [ 0.0253, -0.0386, -0.0294,  ..., -0.0249, -0.0236, -0.0066],\n",
       "          [ 0.0170,  0.0281, -0.0128,  ..., -0.0159,  0.0252,  0.0091],\n",
       "          [-0.0163,  0.0239, -0.0185,  ...,  0.0242, -0.0194,  0.0089]],\n",
       "         requires_grad=True)), ('input_linear.bias', Parameter containing:\n",
       "  tensor([-0.0179, -0.0024, -0.0087, -0.0024,  0.0281, -0.0200,  0.0011, -0.0042,\n",
       "          -0.0110,  0.0131, -0.0274,  0.0019, -0.0290,  0.0364, -0.0308, -0.0477,\n",
       "          -0.0260,  0.0139, -0.0133, -0.0409,  0.0174, -0.0344, -0.0195,  0.0039,\n",
       "          -0.0317, -0.0213,  0.0136,  0.0107, -0.0066, -0.0178,  0.0187, -0.0356,\n",
       "          -0.0203, -0.0260,  0.0179, -0.0087, -0.0213, -0.0101,  0.0258, -0.0329,\n",
       "          -0.0215,  0.0023, -0.0083,  0.0033, -0.0312, -0.0075,  0.0069, -0.0136,\n",
       "          -0.0137, -0.0078, -0.0212,  0.0292,  0.0102, -0.0080, -0.0011, -0.0363,\n",
       "          -0.0034,  0.0019,  0.0113, -0.0055,  0.0210, -0.0298, -0.0074,  0.0307,\n",
       "          -0.0198,  0.0002, -0.0227, -0.0295,  0.0039, -0.0169,  0.0021,  0.0125,\n",
       "          -0.0283, -0.0171, -0.0086, -0.0002,  0.0046, -0.0073,  0.0131,  0.0131,\n",
       "          -0.0223,  0.0077, -0.0249, -0.0094, -0.0156, -0.0322, -0.0359, -0.0087,\n",
       "          -0.0058, -0.0208,  0.0126, -0.0080, -0.0322, -0.0437,  0.0251, -0.0277,\n",
       "          -0.0232,  0.0163,  0.0283,  0.0163], requires_grad=True)), ('middle_linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0064, -0.0216, -0.0712,  ..., -0.0565,  0.1085,  0.0666],\n",
       "          [ 0.0478,  0.0407, -0.0743,  ...,  0.1000,  0.0105, -0.1117],\n",
       "          [-0.0345,  0.0145, -0.0096,  ...,  0.0104,  0.0039,  0.0968],\n",
       "          ...,\n",
       "          [ 0.0945,  0.0490, -0.0592,  ...,  0.0279,  0.0577, -0.0376],\n",
       "          [ 0.0513,  0.0725, -0.1071,  ...,  0.0955,  0.0566, -0.0329],\n",
       "          [-0.0623, -0.0859,  0.0642,  ..., -0.0288, -0.0950,  0.0694]],\n",
       "         requires_grad=True)), ('middle_linear.bias', Parameter containing:\n",
       "  tensor([-0.0466, -0.0369,  0.0610,  0.0834, -0.0023, -0.0782, -0.0013,  0.0958,\n",
       "           0.0716,  0.0245, -0.0556,  0.0711,  0.0042, -0.0036, -0.0615, -0.0166,\n",
       "          -0.0185,  0.0172, -0.0022,  0.0731, -0.0088, -0.0590, -0.0153, -0.0204,\n",
       "           0.0552, -0.0052, -0.0702, -0.0562,  0.0030, -0.0650,  0.0743, -0.1052,\n",
       "           0.1072, -0.0928,  0.0047, -0.0130, -0.0248, -0.0373,  0.0630, -0.0856,\n",
       "          -0.0003,  0.0988,  0.0268, -0.0198,  0.1081,  0.0913,  0.0036, -0.0525,\n",
       "           0.1015,  0.0219,  0.0786,  0.0534, -0.0994, -0.0405, -0.0441,  0.0645,\n",
       "          -0.0368,  0.0437,  0.1147, -0.0597,  0.1039, -0.0746, -0.0335,  0.0687,\n",
       "           0.1109,  0.1183,  0.0218,  0.0081,  0.0477,  0.0068,  0.0522, -0.0622,\n",
       "           0.0218,  0.1058, -0.0396,  0.0028,  0.1612,  0.0176, -0.0173,  0.1210,\n",
       "           0.0549,  0.0370, -0.0840,  0.0423, -0.0409,  0.0934,  0.0263, -0.0200,\n",
       "          -0.0252,  0.0154,  0.0262,  0.0845,  0.0286,  0.1229,  0.0189, -0.0852,\n",
       "           0.0679,  0.0087,  0.0324, -0.0349], requires_grad=True)), ('output_linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0458,  0.0901,  0.0241,  0.0905,  0.0517, -0.0492,  0.1629, -0.0786,\n",
       "            0.0649, -0.0308, -0.0016, -0.0219, -0.0220,  0.0540,  0.1020,  0.0404,\n",
       "            0.0333,  0.0579, -0.0013, -0.0124, -0.1011, -0.0764, -0.0622, -0.1163,\n",
       "            0.0506, -0.0105,  0.1071,  0.0518, -0.0018, -0.0287, -0.0749, -0.0343,\n",
       "           -0.1862, -0.0235, -0.0993,  0.1411, -0.1023, -0.0810,  0.0403,  0.1102,\n",
       "           -0.1369,  0.1197, -0.1202,  0.0998,  0.1013,  0.0902, -0.1984,  0.0658,\n",
       "            0.1597,  0.0486, -0.0484, -0.0288, -0.0569,  0.0655,  0.0353,  0.0907,\n",
       "           -0.0420,  0.0890, -0.0328,  0.0383, -0.0312, -0.0966,  0.0336,  0.1133,\n",
       "           -0.0870,  0.0412,  0.0404,  0.0156, -0.1569, -0.1085, -0.0984, -0.0304,\n",
       "            0.0193, -0.1407, -0.0703,  0.1087, -0.1647, -0.1105, -0.0789,  0.2063,\n",
       "            0.0716,  0.0736,  0.0376, -0.0750,  0.2264, -0.0316,  0.0755,  0.0328,\n",
       "            0.1560,  0.0791, -0.0414,  0.0446,  0.1018, -0.0496,  0.0097, -0.0112,\n",
       "           -0.1225,  0.0581, -0.0344,  0.0012],\n",
       "          [-0.0680,  0.0602,  0.0010,  0.1184, -0.0074, -0.0279, -0.0265,  0.0302,\n",
       "           -0.0760, -0.1472,  0.0591, -0.0328,  0.0208,  0.0163, -0.0415,  0.0614,\n",
       "           -0.0218,  0.0008, -0.0364, -0.1747,  0.1675, -0.0304, -0.0272,  0.0126,\n",
       "           -0.0024,  0.0191, -0.0791, -0.1804, -0.0396,  0.1837,  0.1135,  0.0148,\n",
       "            0.1314, -0.0259,  0.0679, -0.0546, -0.0642, -0.0173, -0.1440, -0.1840,\n",
       "           -0.0441,  0.0865, -0.0255,  0.1877, -0.0527, -0.0509, -0.0606, -0.1132,\n",
       "           -0.1308,  0.0204,  0.0406, -0.0650,  0.0439, -0.0528, -0.0468,  0.2523,\n",
       "           -0.0123, -0.0969,  0.0622,  0.0582,  0.0234, -0.0827,  0.1200,  0.0430,\n",
       "            0.0321,  0.0581, -0.1210, -0.0558,  0.1629,  0.1674, -0.1604,  0.0779,\n",
       "           -0.1839,  0.0094, -0.1247, -0.1142, -0.0451, -0.0682,  0.1709,  0.0975,\n",
       "           -0.0911,  0.0183,  0.0211, -0.0146, -0.0752,  0.1168,  0.0369,  0.1332,\n",
       "           -0.0003, -0.0057,  0.0128, -0.0963, -0.0033, -0.0114, -0.1491, -0.0861,\n",
       "            0.1415, -0.1100, -0.0013,  0.0022],\n",
       "          [ 0.0542, -0.0106, -0.0714,  0.0434,  0.0759, -0.0874, -0.0627, -0.0743,\n",
       "            0.0439,  0.1473,  0.0584, -0.0017,  0.0261,  0.2155,  0.1728,  0.0978,\n",
       "            0.0509, -0.0405,  0.0333,  0.1784,  0.0058,  0.0585,  0.0470, -0.0890,\n",
       "            0.0962, -0.0206, -0.0801, -0.0102,  0.0246, -0.0823, -0.0308, -0.1350,\n",
       "           -0.0184,  0.0190,  0.0853, -0.1413,  0.1251,  0.1205, -0.0306,  0.0773,\n",
       "            0.1369,  0.1704, -0.0968, -0.1316, -0.0785,  0.1395, -0.0713,  0.0968,\n",
       "           -0.2119,  0.0356, -0.1011, -0.0781,  0.0746, -0.0887, -0.0305,  0.0941,\n",
       "           -0.0676, -0.0873, -0.0354,  0.0266, -0.1823, -0.0782, -0.0790, -0.1557,\n",
       "            0.1417, -0.0682,  0.0846, -0.0713, -0.0768, -0.0659,  0.0315,  0.1075,\n",
       "            0.0595, -0.1210,  0.0149,  0.1016, -0.1364,  0.0726,  0.0816,  0.1497,\n",
       "            0.1187, -0.0940,  0.0929,  0.0491, -0.1318, -0.0989, -0.0379,  0.0547,\n",
       "           -0.0172,  0.1424,  0.0719, -0.0426, -0.0652, -0.0085, -0.0593,  0.1174,\n",
       "            0.0131,  0.1024,  0.0178,  0.0024],\n",
       "          [-0.0366, -0.1199,  0.1227,  0.0708, -0.0366,  0.0197,  0.0323,  0.0471,\n",
       "           -0.0532,  0.1112, -0.1152, -0.1520, -0.0803, -0.0583, -0.0068, -0.0260,\n",
       "           -0.0110, -0.0401, -0.0530, -0.0318, -0.0880, -0.0637, -0.0034, -0.0287,\n",
       "           -0.0718,  0.0834, -0.1005,  0.0358, -0.1304, -0.0573, -0.0802,  0.0328,\n",
       "            0.0603,  0.0998, -0.0349,  0.0571,  0.0729, -0.0239, -0.1551,  0.0304,\n",
       "           -0.1638, -0.0090, -0.0477,  0.0228,  0.1767, -0.1417, -0.0003, -0.0551,\n",
       "           -0.0970,  0.1726, -0.1004, -0.1572,  0.0339, -0.0864, -0.0331, -0.1517,\n",
       "           -0.0740,  0.0274, -0.1394,  0.0633,  0.1344,  0.0495, -0.0993,  0.1064,\n",
       "            0.0980, -0.0811, -0.0355,  0.0073, -0.0193, -0.0575,  0.0924, -0.0160,\n",
       "           -0.0730, -0.1163,  0.0493,  0.0196,  0.0228, -0.0146, -0.0360,  0.0473,\n",
       "            0.1984,  0.1140,  0.0396, -0.0736, -0.0464,  0.0509,  0.0447,  0.1387,\n",
       "           -0.0455,  0.0720, -0.0463,  0.0349, -0.0195,  0.0372,  0.0663, -0.0645,\n",
       "            0.1317, -0.0504, -0.1864,  0.1368],\n",
       "          [ 0.0184,  0.1548,  0.0774,  0.1380, -0.1128,  0.0159, -0.0932, -0.1638,\n",
       "            0.0609, -0.0799,  0.0086,  0.0306, -0.0581,  0.0861,  0.0219, -0.1510,\n",
       "            0.1174, -0.0176, -0.1068, -0.1400, -0.0877,  0.1244,  0.0430,  0.0614,\n",
       "            0.0083, -0.0908, -0.1361,  0.1050,  0.0581, -0.0630, -0.0881, -0.1233,\n",
       "           -0.0785,  0.0769,  0.1304, -0.0646,  0.0691, -0.0364, -0.0968,  0.0331,\n",
       "           -0.1166, -0.1133, -0.1035, -0.0544,  0.0825, -0.0172,  0.0293, -0.0278,\n",
       "            0.0190,  0.0334, -0.0734,  0.0341, -0.0300,  0.1377, -0.0848, -0.0421,\n",
       "            0.0753,  0.1338,  0.0867, -0.1111, -0.0524,  0.0700,  0.0325,  0.0543,\n",
       "            0.0485,  0.0494, -0.0008, -0.0875,  0.2085, -0.1163, -0.0226,  0.0333,\n",
       "            0.1029,  0.1471,  0.1239, -0.0010,  0.0068,  0.1326,  0.0081,  0.1288,\n",
       "            0.1134, -0.0685,  0.0963,  0.0386,  0.1107, -0.0598,  0.0016, -0.2072,\n",
       "           -0.0534,  0.0032,  0.2650,  0.2153, -0.2305, -0.1726, -0.0945,  0.0897,\n",
       "            0.0480, -0.0338,  0.0412, -0.0962],\n",
       "          [ 0.0900, -0.0352, -0.0071,  0.0251, -0.0414, -0.0487,  0.0004,  0.1274,\n",
       "            0.1418, -0.0061,  0.0145, -0.1264,  0.0394,  0.0639,  0.0480,  0.0889,\n",
       "            0.0474, -0.0806, -0.0417,  0.0201,  0.1285,  0.0626,  0.0423,  0.0446,\n",
       "           -0.1167, -0.0383, -0.0406,  0.0021,  0.0802,  0.0932, -0.1368, -0.0993,\n",
       "            0.0263,  0.1190, -0.0713, -0.0146, -0.0789,  0.1569,  0.0458, -0.1592,\n",
       "           -0.0957, -0.0277,  0.0736,  0.0256,  0.1466,  0.0018, -0.1178,  0.0941,\n",
       "           -0.1837,  0.0661,  0.0393,  0.1071,  0.0265, -0.0148,  0.0602,  0.0790,\n",
       "           -0.0506, -0.0800, -0.0276, -0.0307,  0.1052, -0.0477,  0.0598, -0.0314,\n",
       "            0.0368,  0.0055, -0.0675, -0.1219, -0.0258, -0.0614,  0.0014, -0.0059,\n",
       "           -0.1227, -0.0086, -0.0497,  0.1870,  0.1471, -0.0566, -0.1073, -0.0525,\n",
       "            0.0008, -0.0746,  0.0830,  0.0692, -0.0037, -0.0070,  0.1961,  0.1237,\n",
       "           -0.0347, -0.0865,  0.1471, -0.1678, -0.0416,  0.0549,  0.0852,  0.0575,\n",
       "           -0.1160,  0.0612,  0.0539,  0.0193],\n",
       "          [ 0.1045,  0.0290,  0.0567, -0.0143,  0.0305,  0.0830,  0.0068, -0.0375,\n",
       "           -0.1076,  0.1055, -0.0247,  0.1830,  0.0614,  0.0416,  0.1122, -0.0342,\n",
       "            0.1069, -0.1008,  0.0403,  0.0155, -0.0650, -0.0107, -0.0739,  0.1154,\n",
       "           -0.0249, -0.0039,  0.0879, -0.0441, -0.0469,  0.0059, -0.0566, -0.0158,\n",
       "           -0.0619,  0.0639, -0.1156,  0.1629,  0.1397,  0.1828, -0.0878, -0.1387,\n",
       "            0.0437,  0.0763, -0.1232, -0.0358,  0.0528,  0.1168,  0.0786,  0.1405,\n",
       "           -0.0213,  0.1382, -0.0494, -0.0688, -0.0118,  0.0760, -0.0218,  0.0654,\n",
       "           -0.0674,  0.1264, -0.1081, -0.0017, -0.0757, -0.1138,  0.0681,  0.0611,\n",
       "            0.0690, -0.0478,  0.0377, -0.0489, -0.0087,  0.0466,  0.0501, -0.0284,\n",
       "            0.0321, -0.2027,  0.0855, -0.1072,  0.1188, -0.0042, -0.0593, -0.1056,\n",
       "           -0.0477, -0.0238, -0.0921, -0.1980,  0.0812, -0.0635, -0.1108, -0.0558,\n",
       "           -0.0448,  0.0190, -0.1178,  0.0858, -0.0627, -0.0190, -0.1429,  0.0255,\n",
       "           -0.0757,  0.1052,  0.0681,  0.0871],\n",
       "          [ 0.0168, -0.0819, -0.0748,  0.0374, -0.0477,  0.0023, -0.0053, -0.0661,\n",
       "            0.0486,  0.0340, -0.0693,  0.1703, -0.1153,  0.0772,  0.0736, -0.0947,\n",
       "            0.0737,  0.0639, -0.0481,  0.0567, -0.0178, -0.1298, -0.1019,  0.0436,\n",
       "           -0.0297,  0.1364,  0.0216,  0.0960,  0.0814, -0.0442, -0.0153,  0.0007,\n",
       "            0.0573,  0.1285,  0.0031, -0.0251, -0.1578, -0.0536, -0.1301,  0.0715,\n",
       "           -0.0177,  0.1966, -0.1230, -0.1392,  0.1290, -0.2115, -0.1318, -0.1119,\n",
       "            0.0036, -0.0620,  0.0695, -0.0961,  0.0662, -0.1821,  0.1303, -0.0263,\n",
       "            0.1103,  0.0590,  0.0672,  0.0246, -0.0938, -0.0561,  0.1150,  0.1708,\n",
       "            0.0493, -0.1049, -0.0490, -0.0969,  0.0293,  0.0676, -0.0156,  0.1278,\n",
       "           -0.0346, -0.0581,  0.0666,  0.0728,  0.1127,  0.1332,  0.0403,  0.0805,\n",
       "           -0.0401, -0.2038, -0.1353, -0.0328, -0.0665, -0.0511, -0.1250,  0.0128,\n",
       "            0.0616, -0.1040, -0.0718, -0.0083, -0.1006,  0.2901, -0.0147, -0.0933,\n",
       "            0.1194,  0.0265,  0.0967, -0.0377],\n",
       "          [ 0.0314, -0.0132, -0.0093,  0.2042, -0.0350,  0.0087,  0.0285, -0.0167,\n",
       "            0.1429,  0.0732,  0.1565, -0.1778, -0.0197,  0.0976,  0.0928, -0.0243,\n",
       "            0.0607,  0.1002, -0.0175, -0.0882,  0.0116,  0.1226,  0.0231, -0.1466,\n",
       "           -0.0287, -0.0296, -0.0625,  0.1188, -0.1258, -0.0424,  0.2253, -0.0401,\n",
       "           -0.1058, -0.0742,  0.0328, -0.0072, -0.0281, -0.0922, -0.0394,  0.1263,\n",
       "            0.0868, -0.0206,  0.0015,  0.0627, -0.1854, -0.0499,  0.0310,  0.0232,\n",
       "           -0.0319,  0.1399,  0.0981,  0.0753, -0.0092, -0.1455,  0.0660,  0.0749,\n",
       "            0.0203,  0.0993, -0.1320,  0.0453, -0.1051, -0.0083, -0.0635,  0.0064,\n",
       "           -0.0058,  0.0952,  0.0552, -0.0130, -0.0569,  0.0012, -0.0771, -0.1497,\n",
       "           -0.0627, -0.1501, -0.1069, -0.0035, -0.0367,  0.1358, -0.0053, -0.0559,\n",
       "           -0.0744, -0.0427, -0.1209,  0.0425, -0.0708,  0.0001,  0.0699,  0.0291,\n",
       "            0.0589, -0.0517,  0.1021,  0.2554,  0.1974,  0.0636, -0.0668, -0.1059,\n",
       "           -0.1897, -0.0500,  0.0991,  0.0068],\n",
       "          [ 0.0120,  0.0689,  0.0831, -0.0523,  0.0208, -0.0505, -0.0400,  0.0650,\n",
       "           -0.0625,  0.1838,  0.0253, -0.0103,  0.0954, -0.0808,  0.0887, -0.0197,\n",
       "            0.0071,  0.0713,  0.0315, -0.0769,  0.0078,  0.1791,  0.0079, -0.0839,\n",
       "           -0.0064,  0.1475, -0.0002, -0.0534,  0.0363,  0.1055,  0.0837, -0.2323,\n",
       "           -0.0947,  0.0247,  0.0701, -0.0900, -0.0578, -0.0505,  0.0304,  0.0584,\n",
       "            0.0212, -0.0615, -0.0232, -0.0520, -0.0891,  0.0506, -0.1493,  0.0731,\n",
       "            0.0426, -0.0576, -0.0723, -0.0156, -0.1056,  0.0341, -0.0180, -0.0078,\n",
       "           -0.0586, -0.0726,  0.1664, -0.0101, -0.1402, -0.0567, -0.0147,  0.1546,\n",
       "           -0.1962,  0.0919, -0.0999, -0.0650,  0.0765, -0.0064, -0.1451, -0.1305,\n",
       "            0.0786,  0.0149,  0.0028,  0.1504,  0.2032, -0.0437,  0.0306, -0.0143,\n",
       "            0.0249,  0.1141, -0.0224, -0.1759,  0.2042,  0.0815, -0.1464,  0.1214,\n",
       "           -0.1064,  0.0387,  0.0398, -0.0428, -0.2047, -0.0160,  0.0467,  0.0005,\n",
       "           -0.0277,  0.1223, -0.0406, -0.1163]], requires_grad=True)), ('output_linear.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0761,  0.2427, -0.0440, -0.0289, -0.1280,  0.0046,  0.0296,  0.0400,\n",
       "          -0.0109, -0.0083], requires_grad=True))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input_linear.weight', Parameter containing:\n",
       " tensor([[ 0.0061,  0.0238, -0.0026,  ...,  0.0098, -0.0177,  0.0157],\n",
       "         [-0.0202, -0.0154, -0.0319,  ...,  0.0318,  0.0074,  0.0169],\n",
       "         [-0.0454,  0.0056,  0.0340,  ..., -0.0133,  0.0121, -0.0135],\n",
       "         ...,\n",
       "         [ 0.0253, -0.0386, -0.0294,  ..., -0.0249, -0.0236, -0.0066],\n",
       "         [ 0.0170,  0.0281, -0.0128,  ..., -0.0159,  0.0252,  0.0091],\n",
       "         [-0.0163,  0.0239, -0.0185,  ...,  0.0242, -0.0194,  0.0089]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, transformations=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = np.random.randn(10, 10)\n",
    "        self.transformations = transformations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = {'random_vector': self.data[index]}\n",
    "        if self.transformations:\n",
    "            sample = self.transformations(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        random_vector = sample['random_vector']\n",
    "\n",
    "        return {'random_vector': torch.from_numpy(random_vector)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDataset(transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_vector': tensor([ 0.1416, -1.2562, -0.9917, -1.7103,  1.9722, -1.2794, -0.8276,  1.9544,\n",
       "         -0.1124,  1.5208], dtype=torch.float64)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=cd, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_vector': tensor([[ 1.0592,  0.1721,  0.1390,  0.8516,  0.3061, -0.6238,  0.2344, -1.8680,\n",
       "          -0.0221,  1.6277],\n",
       "         [-1.3695,  0.2851, -1.7845,  0.1750,  0.0251,  0.7172,  1.6801,  0.3078,\n",
       "           1.6139, -1.5179],\n",
       "         [-0.7740,  0.9719, -0.1336, -0.3983,  0.0529, -1.7813,  1.3427, -0.1581,\n",
       "           2.3445, -1.7506],\n",
       "         [-0.8614,  0.8886, -0.4223,  0.1397, -3.1475,  0.7675,  1.4525,  0.1545,\n",
       "           3.0248, -1.2721]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
