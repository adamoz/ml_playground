{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload v2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {x: datasets.ImageFolder(os.path.join(root_folder, x), transformation[x]) for x in ['train', 'valid']}\n",
    "loader = {x: torch.utils.data.DataLoader(dataset[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
    "\n",
    "dataset_size = {x: len(dataset[x]) for x in ['train', 'valid']}\n",
    "class_names = dataset['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size = len(dataset['train'].classes)\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet18.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512, output_size),\n",
    "    torch.nn.Softmax(dim=1))\n",
    "\n",
    "loss_fce = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet18.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (1): Softmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_acc_and_loss(model, loss_fce, valid_loader):\n",
    "    accuracy = 0\n",
    "    loss = 0\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    for images, labels in valid_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        predictions = model(images.cuda())\n",
    "        accuracy += (predictions.argmax(dim=1) == labels).type(torch.FloatTensor).mean().item() \n",
    "        loss += loss_fce(predictions, labels).item()\n",
    "    \n",
    "    model.train(mode=was_training)\n",
    "    return accuracy / len(valid_loader) * 100, loss / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Initial params setup.\n",
    "epochs = 5\n",
    "report_period = 3\n",
    "batch_iteration = 0\n",
    "\n",
    "# Storing of some data.\n",
    "train_leak_loss = deque(maxlen=report_period)\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "valid_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Train Loss: 0.57..  Valid Loss: 0.63..  Valid Acc: 66.62%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.62..  Valid Acc: 66.38%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.62..  Valid Acc: 65.88%\n",
      "Epoch: 1/2..  Train Loss: 0.63..  Valid Loss: 0.61..  Valid Acc: 67.5%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.61..  Valid Acc: 67.88%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.61..  Valid Acc: 68.0%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.61..  Valid Acc: 68.75%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.61..  Valid Acc: 68.38%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.61..  Valid Acc: 68.12%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.6..  Valid Acc: 68.25%\n",
      "Epoch: 1/2..  Train Loss: 0.65..  Valid Loss: 0.6..  Valid Acc: 69.25%\n",
      "Epoch: 1/2..  Train Loss: 0.64..  Valid Loss: 0.6..  Valid Acc: 69.88%\n",
      "Epoch: 1/2..  Train Loss: 0.63..  Valid Loss: 0.59..  Valid Acc: 70.25%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.59..  Valid Acc: 68.75%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.59..  Valid Acc: 68.25%\n",
      "Epoch: 1/2..  Train Loss: 0.53..  Valid Loss: 0.59..  Valid Acc: 69.0%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.58..  Valid Acc: 69.88%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.58..  Valid Acc: 70.0%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.57..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 72.62%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 73.0%\n",
      "Epoch: 1/2..  Train Loss: 0.57..  Valid Loss: 0.57..  Valid Acc: 73.25%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 73.0%\n",
      "Epoch: 1/2..  Train Loss: 0.61..  Valid Loss: 0.57..  Valid Acc: 73.25%\n",
      "Epoch: 1/2..  Train Loss: 0.57..  Valid Loss: 0.57..  Valid Acc: 71.62%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.58..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.57..  Valid Acc: 72.25%\n",
      "Epoch: 1/2..  Train Loss: 0.53..  Valid Loss: 0.57..  Valid Acc: 72.75%\n",
      "Epoch: 1/2..  Train Loss: 0.65..  Valid Loss: 0.57..  Valid Acc: 73.62%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.57..  Valid Acc: 73.0%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.57..  Valid Acc: 72.25%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 73.12%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.58..  Valid Acc: 72.88%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.6..  Valid Acc: 70.0%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.61..  Valid Acc: 68.88%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.61..  Valid Acc: 68.88%\n",
      "Epoch: 1/2..  Train Loss: 0.65..  Valid Loss: 0.6..  Valid Acc: 69.25%\n",
      "Epoch: 1/2..  Train Loss: 0.61..  Valid Loss: 0.58..  Valid Acc: 72.5%\n",
      "Epoch: 1/2..  Train Loss: 0.51..  Valid Loss: 0.57..  Valid Acc: 73.25%\n",
      "Epoch: 1/2..  Train Loss: 0.62..  Valid Loss: 0.56..  Valid Acc: 73.25%\n",
      "Epoch: 1/2..  Train Loss: 0.57..  Valid Loss: 0.57..  Valid Acc: 72.88%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.57..  Valid Acc: 73.38%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.56..  Valid Acc: 73.75%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.56..  Valid Acc: 73.88%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 72.0%\n",
      "Epoch: 1/2..  Train Loss: 0.65..  Valid Loss: 0.57..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.57..  Valid Acc: 72.5%\n",
      "Epoch: 1/2..  Train Loss: 0.63..  Valid Loss: 0.58..  Valid Acc: 71.38%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 71.38%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.58..  Valid Acc: 71.0%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.58..  Valid Acc: 71.75%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 72.12%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.58..  Valid Acc: 70.75%\n",
      "Epoch: 1/2..  Train Loss: 0.63..  Valid Loss: 0.57..  Valid Acc: 71.75%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 71.5%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 72.5%\n",
      "Epoch: 1/2..  Train Loss: 0.57..  Valid Loss: 0.57..  Valid Acc: 72.25%\n",
      "Epoch: 1/2..  Train Loss: 0.56..  Valid Loss: 0.57..  Valid Acc: 70.88%\n",
      "Epoch: 1/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.58..  Valid Acc: 70.75%\n",
      "Epoch: 1/2..  Train Loss: 0.58..  Valid Loss: 0.58..  Valid Acc: 70.75%\n",
      "Epoch: 1/2..  Train Loss: 0.6..  Valid Loss: 0.58..  Valid Acc: 70.75%\n",
      "Epoch: 1/2..  Train Loss: 0.53..  Valid Loss: 0.58..  Valid Acc: 71.62%\n",
      "Epoch: 1/2..  Train Loss: 0.61..  Valid Loss: 0.57..  Valid Acc: 71.88%\n",
      "Epoch: 1/2..  Train Loss: 0.61..  Valid Loss: 0.56..  Valid Acc: 73.38%\n",
      "Epoch: 1/2..  Train Loss: 0.63..  Valid Loss: 0.57..  Valid Acc: 72.12%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 71.62%\n",
      "Epoch: 1/2..  Train Loss: 0.52..  Valid Loss: 0.58..  Valid Acc: 70.88%\n",
      "Epoch: 1/2..  Train Loss: 0.7..  Valid Loss: 0.57..  Valid Acc: 71.5%\n",
      "Epoch: 1/2..  Train Loss: 0.61..  Valid Loss: 0.58..  Valid Acc: 70.88%\n",
      "Epoch: 1/2..  Train Loss: 0.52..  Valid Loss: 0.57..  Valid Acc: 72.25%\n",
      "Epoch: 1/2..  Train Loss: 0.53..  Valid Loss: 0.56..  Valid Acc: 73.25%\n",
      "Epoch: 1/2..  Train Loss: 0.65..  Valid Loss: 0.56..  Valid Acc: 72.75%\n",
      "Epoch: 1/2..  Train Loss: 0.55..  Valid Loss: 0.56..  Valid Acc: 73.12%\n",
      "Epoch: 1/2..  Train Loss: 0.52..  Valid Loss: 0.56..  Valid Acc: 73.62%\n",
      "Epoch: 2/2..  Train Loss: 0.58..  Valid Loss: 0.56..  Valid Acc: 73.75%\n",
      "Epoch: 2/2..  Train Loss: 0.6..  Valid Loss: 0.56..  Valid Acc: 73.38%\n",
      "Epoch: 2/2..  Train Loss: 0.53..  Valid Loss: 0.56..  Valid Acc: 73.62%\n",
      "Epoch: 2/2..  Train Loss: 0.52..  Valid Loss: 0.56..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.56..  Valid Acc: 74.12%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.59..  Valid Loss: 0.57..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.54..  Valid Loss: 0.57..  Valid Acc: 72.5%\n",
      "Epoch: 2/2..  Train Loss: 0.65..  Valid Loss: 0.58..  Valid Acc: 72.5%\n",
      "Epoch: 2/2..  Train Loss: 0.51..  Valid Loss: 0.57..  Valid Acc: 73.25%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.57..  Valid Acc: 74.0%\n",
      "Epoch: 2/2..  Train Loss: 0.56..  Valid Loss: 0.56..  Valid Acc: 74.0%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.56..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.56..  Valid Acc: 75.0%\n",
      "Epoch: 2/2..  Train Loss: 0.56..  Valid Loss: 0.56..  Valid Acc: 73.5%\n",
      "Epoch: 2/2..  Train Loss: 0.49..  Valid Loss: 0.57..  Valid Acc: 72.62%\n",
      "Epoch: 2/2..  Train Loss: 0.62..  Valid Loss: 0.57..  Valid Acc: 73.62%\n",
      "Epoch: 2/2..  Train Loss: 0.6..  Valid Loss: 0.56..  Valid Acc: 73.5%\n",
      "Epoch: 2/2..  Train Loss: 0.6..  Valid Loss: 0.56..  Valid Acc: 74.38%\n",
      "Epoch: 2/2..  Train Loss: 0.48..  Valid Loss: 0.55..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.55..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.61..  Valid Loss: 0.56..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.51..  Valid Loss: 0.56..  Valid Acc: 74.12%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.55..  Valid Acc: 74.38%\n",
      "Epoch: 2/2..  Train Loss: 0.61..  Valid Loss: 0.55..  Valid Acc: 75.62%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.55..  Valid Acc: 75.12%\n",
      "Epoch: 2/2..  Train Loss: 0.55..  Valid Loss: 0.55..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.59..  Valid Loss: 0.55..  Valid Acc: 74.38%\n",
      "Epoch: 2/2..  Train Loss: 0.54..  Valid Loss: 0.56..  Valid Acc: 73.62%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.55..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.53..  Valid Loss: 0.55..  Valid Acc: 74.25%\n",
      "Epoch: 2/2..  Train Loss: 0.54..  Valid Loss: 0.55..  Valid Acc: 75.0%\n",
      "Epoch: 2/2..  Train Loss: 0.62..  Valid Loss: 0.55..  Valid Acc: 75.62%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.55..  Valid Acc: 74.5%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.56..  Valid Acc: 73.75%\n",
      "Epoch: 2/2..  Train Loss: 0.56..  Valid Loss: 0.56..  Valid Acc: 73.62%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.56..  Valid Acc: 74.0%\n",
      "Epoch: 2/2..  Train Loss: 0.57..  Valid Loss: 0.56..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.51..  Valid Loss: 0.56..  Valid Acc: 73.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2..  Train Loss: 0.51..  Valid Loss: 0.56..  Valid Acc: 73.75%\n",
      "Epoch: 2/2..  Train Loss: 0.56..  Valid Loss: 0.56..  Valid Acc: 73.5%\n",
      "Epoch: 2/2..  Train Loss: 0.6..  Valid Loss: 0.56..  Valid Acc: 73.88%\n",
      "Epoch: 2/2..  Train Loss: 0.61..  Valid Loss: 0.56..  Valid Acc: 73.75%\n",
      "Epoch: 2/2..  Train Loss: 0.67..  Valid Loss: 0.56..  Valid Acc: 73.38%\n",
      "Epoch: 2/2..  Train Loss: 0.58..  Valid Loss: 0.55..  Valid Acc: 74.38%\n",
      "Epoch: 2/2..  Train Loss: 0.49..  Valid Loss: 0.55..  Valid Acc: 75.12%\n",
      "Epoch: 2/2..  Train Loss: 0.61..  Valid Loss: 0.55..  Valid Acc: 74.5%\n",
      "Epoch: 2/2..  Train Loss: 0.51..  Valid Loss: 0.55..  Valid Acc: 74.5%\n",
      "Epoch: 2/2..  Train Loss: 0.58..  Valid Loss: 0.56..  Valid Acc: 74.0%\n",
      "Epoch: 2/2..  Train Loss: 0.59..  Valid Loss: 0.56..  Valid Acc: 74.0%\n",
      "Epoch: 2/2..  Train Loss: 0.6..  Valid Loss: 0.56..  Valid Acc: 74.12%\n",
      "Epoch: 2/2..  Train Loss: 0.53..  Valid Loss: 0.56..  Valid Acc: 73.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/adam/.pyenv/versions/3.6.3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cd2509eebea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# We don't want to collect info for gradients from here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_acc_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             print(f'Epoch: {epoch+1}/{epochs}.. ',\n",
      "\u001b[0;32m<ipython-input-23-510b5503a073>\u001b[0m in \u001b[0;36mget_valid_acc_and_loss\u001b[0;34m(model, loss_fce, valid_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Setup net to train mode and go through one epoch.\n",
    "    model.train()\n",
    "    for images, labels in loader['train']:\n",
    "        batch_iteration += 1\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        # Training net on one batch.\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(images)\n",
    "        loss = loss_fce(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_leak_loss.append(loss.item())\n",
    "        # In case we should report, lets make validation on valid set.\n",
    "        if batch_iteration % report_period == 0:\n",
    "            model.eval()\n",
    "            # We don't want to collect info for gradients from here.\n",
    "            with torch.no_grad():\n",
    "                valid_accuracy, valid_loss = get_valid_acc_and_loss(model, loss_fce, loader['valid'])\n",
    "                \n",
    "            print(f'Epoch: {epoch+1}/{epochs}.. ',\n",
    "                  f\"Train Loss: {round(np.mean(train_leak_loss), 2)}.. \",\n",
    "                  f\"Valid Loss: {round(valid_loss, 2)}.. \",\n",
    "                  f\"Valid Acc: {round(valid_accuracy, 2)}%\")\n",
    "            \n",
    "            train_loss_history.append(np.mean(train_leak_loss))\n",
    "            valid_loss_history.append(valid_loss)\n",
    "            valid_acc_history.append(valid_accuracy)\n",
    "                        \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer.param_groups[0]['params'] = optimizer.param_groups[0]['params'] + list(model.layer4.parameters())\n",
    "optimizer.param_groups[0]['lr'] = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = datasets.ImageFolder('./dataset/train', transformation)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TORCH_HOME\"] = \"./\"\n",
    "info = model.eval()\n",
    "\n",
    "resnet18_features = torch.nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 20/228 [00:21<03:33,  1.02s/it]"
     ]
    }
   ],
   "source": [
    "features = list()\n",
    "labels = list()\n",
    "images = list()\n",
    "idx = 0\n",
    "for img, label in tqdm.tqdm(loader):\n",
    "    features += resnet18_features(img).squeeze().detach().numpy().tolist()\n",
    "    labels += list(map(lambda l: dataset.classes[l], label.squeeze().detach().numpy().tolist()))\n",
    "    \n",
    "    img = img.squeeze().detach().numpy()\n",
    "    img = np.transpose(img, (0, 2, 3, 1))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = np.transpose(img, (0, 3, 1, 2))\n",
    "    images += img.tolist()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx > 20:\n",
    "        break\n",
    "    \n",
    "    \n",
    "features = torch.tensor(np.array(features))\n",
    "images = torch.tensor(np.array(images)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(resnet18_features, iter(loader).__next__()[0])\n",
    "\n",
    "writer.add_embedding(mat=features, metadata=labels, label_img=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
